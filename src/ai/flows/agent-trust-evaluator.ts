// This is an autogenerated file from Firebase Studio.

'use server';

/**
 * @fileOverview This file defines a Genkit flow for evaluating the trust of an agent based on its behavior, 
 * identifying anomalies and potential security risks.
 *
 * The flow takes agent activity data as input and returns a trust score with justifications.
 *
 * @requires genkit - Genkit for defining flows and prompts.
 * @requires zod - Zod for schema validation.
 *
 * @exports evaluateAgentTrust - A function that evaluates the trust of an agent.
 * @exports AgentTrustEvaluatorInput - The input type for the evaluateAgentTrust function.
 * @exports AgentTrustEvaluatorOutput - The output type for the evaluateAgentTrust function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

/**
 * Input schema for the Agent Trust Evaluator flow.
 */
const AgentTrustEvaluatorInputSchema = z.object({
  agentId: z.string().describe('The unique identifier of the agent.'),
  agentActivityData: z.string().describe('A string containing agent activity data, including actions, resource access, and communication logs.'),
  threatLandscapeDescription: z.string().describe('Description of the current threat landscape, including recent attack vectors and vulnerabilities.'),
});

export type AgentTrustEvaluatorInput = z.infer<typeof AgentTrustEvaluatorInputSchema>;

/**
 * Output schema for the Agent Trust Evaluator flow.
 */
const AgentTrustEvaluatorOutputSchema = z.object({
  trustScore: z.number().describe('A numerical score representing the agent\u0027s trustworthiness (0-1).'),
  justification: z.string().describe('Explanation of how the trust score was determined, including identified anomalies and deviations.'),
});

export type AgentTrustEvaluatorOutput = z.infer<typeof AgentTrustEvaluatorOutputSchema>;

/**
 * Async wrapper function to call the flow.
 * @param input
 * @returns {Promise<AgentTrustEvaluatorOutput>}
 */
export async function evaluateAgentTrust(input: AgentTrustEvaluatorInput): Promise<AgentTrustEvaluatorOutput> {
  return agentTrustEvaluatorFlow(input);
}

/**
 * Prompt definition for the Agent Trust Evaluator.
 */
const agentTrustEvaluatorPrompt = ai.definePrompt({
  name: 'agentTrustEvaluatorPrompt',
  input: {schema: AgentTrustEvaluatorInputSchema},
  output: {schema: AgentTrustEvaluatorOutputSchema},
  prompt: `You are a security expert evaluating the trustworthiness of an AI agent.

  Analyze the provided agent activity data in the context of the current threat landscape to identify anomalies,
  deviations from established patterns, and potential security risks. Provide a trust score between 0 and 1,
  where 0 indicates very low trust and 1 indicates very high trust.  Justify the assigned trust score with a
  clear explanation.

  Agent ID: {{{agentId}}}
  Agent Activity Data: {{{agentActivityData}}}
  Threat Landscape Description: {{{threatLandscapeDescription}}}

  Consider the following factors when determining the trust score and justification:
  - Unusual or unauthorized resource access
  - Deviations from expected communication patterns
  - Anomalous behavior indicative of compromise
  - Vulnerabilities exposed by the current threat landscape

  Provide your evaluation in the following format:
  {
  "trustScore": number,
  "justification": string
  }
  `,
});

/**
 * Genkit flow definition for evaluating agent trust.
 */
const agentTrustEvaluatorFlow = ai.defineFlow(
  {
    name: 'agentTrustEvaluatorFlow',
    inputSchema: AgentTrustEvaluatorInputSchema,
    outputSchema: AgentTrustEvaluatorOutputSchema,
  },
  async input => {
    const {output} = await agentTrustEvaluatorPrompt(input);
    return output!;
  }
);
